{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning XLSR-Wav2Vec2 for Korean ASR with ğŸ¤— Transformers\n",
    "\n",
    "> ë³¸ íŠœí† ë¦¬ì–¼ì€ [Fine-tuning XLS-R for Multi-Lingual ASR with ğŸ¤— Transformers](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Fine_Tune_XLS_R_on_Common_Voice.ipynb)ì„ ì°¸ê³ í•˜ì—¬ ì œì‘ë˜ì—ˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wav2Vec2.0\n",
    "\n",
    "- https://ai.meta.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/\n",
    "- Wav2Vec2ëŠ” ìë™ ìŒì„± ì¸ì‹(ASR)ì„ ìœ„í•œ pre-trained modelë¡œ 2020ë…„ 9ì›” Alexei Baevski, Michael Auli, Alex Conneauê°€ ì œì‹œ \n",
    "- 53000 ì‹œê°„ì˜ ë¼ë²¨ë§ ì—†ëŠ” ë°ì´í„°ë¡œ representation trainingë˜ì–´ ì†ŒëŸ‰ì˜ ë¼ë²¨ë§ ëœ ë°ì´í„°ë¡œ ìŒì„±ì¸ì‹ ê´€ë ¨ down-stream taskì— fine-tuning\n",
    "- Librispeechì— ëŒ€í•´ì„œ noise dataëŠ” WER 8.6%, clean dataëŠ” WER 5.2%\n",
    "\n",
    "<img src= \"https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/xlsr_wav2vec2.png\">\n",
    "\n",
    "## XLSR-Wav2Vec2.0\n",
    "- XLSRì€ 'cross-lingual speech representations'ì˜ ì•½ìë¡œ, ì—¬ëŸ¬ ì–¸ì–´ì— ê±¸ì³ ìœ ìš©í•œ ìŒì„± í‘œí˜„ì„ í•™ìŠµí•  ìˆ˜ ìˆëŠ” XLSR-Wav2Vec2ì˜ ê¸°ëŠ¥ì„ ì˜ë¯¸\n",
    "- Wav2Vec2ì™€ ë§ˆì°¬ê°€ì§€ë¡œ XLSR-Wav2Vec2ëŠ” 50ê°œ ì´ìƒì˜ ì–¸ì–´ë¡œ ëœ ë¼ë²¨ì´ ì—†ëŠ” ìˆ˜ì‹­ë§Œ ì‹œê°„ ë¶„ëŸ‰ì˜ ìŒì„± í•™ìŠµ. \n",
    "- BERTì˜ masked language modelingê³¼ ë§ˆì°¬ê°€ì§€ë¡œ, ì´ ëª¨ë¸ì€ feature vectorë¥¼ ë¬´ì‘ìœ„ë¡œ maskingí•œ í›„ transformer networkì— ì „ë‹¬í•˜ì—¬ ë¬¸ë§¥í™”ëœ ìŒì„± í‘œí˜„ì„ í•™ìŠµ\n",
    "- XLSR-Wav2Vec2ëŠ” ì£¼ë¡œ CTCë¥¼ ì‚¬ìš©í•˜ì—¬ fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "- zeroth_korean ì˜ í•œêµ­ì–´ ë°ì´í„°ì…‹ ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb\n",
    "\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate\n",
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install torchaudio\n",
    "!pip install librosa\n",
    "!pip install jiwer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huggingface login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt install git-lfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\"kresnik/zeroth_korean\", split='train[:1800]')\n",
    "test_dataset = load_dataset(\"kresnik/zeroth_korean\", split='test[:200]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_dataset.remove_columns([\"speaker_id\", \"chapter_id\", \"id\"])\n",
    "test_ds = test_dataset.remove_columns([\"speaker_id\", \"chapter_id\", \"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_ds)\n",
    "print(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "zeroth_korean ë°ì´í„°ì…‹ì€ íŠ¹ìˆ˜ë¬¸ìê°€ í¬í•¨ë˜ì–´ìˆì§€ ì•Šê¸° ë•Œë¬¸ì— transcriptionì— ëŒ€í•œ í›„ì²˜ë¦¬ í•„ìš”ì—†ìŒ\n",
    "\n",
    "CTCì—ì„œëŠ” audio chunkë¥¼ ë¬¸ìë¡œ ë¶„ë¥˜í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì \n",
    "\n",
    "train ë° test dataì˜ ëª¨ë“  characterë¥¼ ì¶”ì¶œí•˜ê³  vocab dictionaryë¥¼ ìƒì„± "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_chars(batch):\n",
    "  all_text = \" \".join(batch[\"text\"])\n",
    "  vocab = list(set(all_text))\n",
    "  return {\"vocab\": [vocab], \"all_text\": [all_text]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_train = train_ds.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=train_ds.column_names)\n",
    "vocab_test = test_ds.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=test_ds.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = list(set(vocab_train[\"vocab\"][0]) | set(vocab_test[\"vocab\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë¹ˆì¹¸ë„ í¬í•¨ëœ ê²ƒì„ í™•ì¸ì´ ê°€ëŠ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = {v : k for k,v in enumerate(vocab_list)}\n",
    "vocab_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`\" \"`ì— ê³ ìœ í•œ í† í° í´ë˜ìŠ¤ê°€ ìˆë‹¤ëŠ” ê²ƒì„ ë” ëª…í™•íˆ í•˜ê¸° ìœ„í•´ ë” ëˆˆì— ì˜ ë„ëŠ” ë¬¸ì `\"|\"`ë¥¼ ë¶€ì—¬\n",
    "\n",
    "`\"[UNK]\"` í† í°ì„ ì¶”ê°€í•˜ì—¬ ëª¨ë¸ì´ ë‚˜ì¤‘ì— zeroth_koreanì˜ train setì—ì„œ ì ‘í•˜ì§€ ëª»í•œ characterë¥¼ ì²˜ë¦¬ ê°€ëŠ¥í•˜ë„ë¡ í•¨\n",
    "\n",
    "CTCì˜ \"BLK\" ë˜ëŠ” \"_\"ì— í•´ë‹¹í•˜ëŠ” `\"[PAD]\"` í† í°ë„ ì¶”ê°€í•©ë‹ˆë‹¤. `\"[PAD]\"`ì€ CTC ì•Œê³ ë¦¬ì¦˜ì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
    "del vocab_dict[\" \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
    "len(vocab_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ 1205ê°œì˜ token: pre-trained XLSR-Wav2Vec2 ì²´í¬í¬ì¸íŠ¸ ìœ„ì— ì¶”ê°€í•  linear layerì˜ output dimensionì˜ í¬ê¸°ê°€ 1205"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "json ìœ¼ë¡œ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('vocab.json', 'w') as vocab_file:\n",
    "    json.dump(vocab_dict, vocab_file, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Feature Extractor, Tokenizer and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Wav2Vec2FeatureExtractor\n",
    "A XLSR-Wav2Vec2 feature extractor object requires the following parameters to be instantiated:\n",
    "\n",
    "- `feature_size`: Speech models take a sequence of feature vectors as an input. While the length of this sequence obviously varies, the feature size should not. In the case of Wav2Vec2, the feature size is 1 because the model was trained on the raw speech signal ${}^2$.\n",
    "- `sampling_rate`: The sampling rate at which the model is trained on.\n",
    "- `padding_value`: For batched inference, shorter inputs need to be padded with a specific value\n",
    "- `do_normalize`: Whether the input should be *zero-mean-unit-variance* normalized or not. Usually, speech models perform better when normalizing the input\n",
    "- `return_attention_mask`: Whether the model should make use of an `attention_mask` for batched inference. In general, XLSR-Wav2Vec2 models should **always** make use of the `attention_mask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Wav2Vec2CTCTokenizer\n",
    "\n",
    "json íŒŒì¼ì„ ì´ìš©í•˜ì—¬ Wav2Vec2CTokenizer í´ë˜ìŠ¤ì˜ ê°ì²´ë¥¼ ì¸ìŠ¤í„´ìŠ¤í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer\n",
    "\n",
    "tokenizer = Wav2Vec2CTCTokenizer(\"./vocab.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine To Create A Wav2VecProcessor\n",
    "\n",
    "feauter extractor ì™€ tokenizerëŠ” Wav2VecProcessor í´ë˜ìŠ¤ë¡œ wrapë˜ì–´ trainì—ì„œëŠ” `processor` ì™€ `model`ë§Œ ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor\n",
    "\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resample to 16kHz\n",
    "\n",
    "`cast_column` : ì˜¤ë””ì˜¤ë¥¼ ì œìë¦¬ì—ì„œ ë³€ê²½í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì˜¤ë””ì˜¤ ìƒ˜í”Œì„ ì²˜ìŒ loadí•  ë•Œ ì¦‰ì‹œ resampleë˜ë„ë¡ Datasetì— ì‹ í˜¸ë¥¼ ë³´ë‚¸ë‹¤. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "\n",
    "train_ds = train_ds.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "test_ds = test_ds.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0ë²ˆ indexì˜ audio sampleì„ reloadí•˜ë©´ 16kHzë¡œ resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Audio Sample í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "rand_int = random.randint(0, len(train_ds)-1)\n",
    "\n",
    "ipd.display(ipd.Audio(data=train_ds[rand_int][\"audio\"][\"array\"], autoplay=True, rate=16000))\n",
    "\n",
    "print(\"Target text:\", train_ds[rand_int][\"text\"])\n",
    "print(\"Input array shape:\", train_ds[rand_int][\"audio\"][\"array\"].shape)\n",
    "print(\"Sampling rate:\", train_ds[rand_int][\"audio\"][\"sampling_rate\"])\n",
    "print(\"Input Duration:\", train_ds[rand_int][\"audio\"][\"array\"].shape[0]/train_ds[rand_int][\"audio\"][\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `batch[\"audio\"]`ë¥¼ í˜¸ì¶œí•˜ì—¬ ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  16kHzë¡œ resampling  \n",
    "\n",
    "2. ë¡œë“œëœ ì˜¤ë””ì˜¤ íŒŒì¼ì—ì„œ `input_values`ê°’ì„ ì¶”ì¶œ ì—¬ê¸°ì—ëŠ” ì •ê·œí™”ë§Œ í¬í•¨(CNN encoderê°€ featureë¥¼ ì¶”ì¶œí•˜ë¯€ë¡œ)ë˜ì§€ë§Œ, ë‹¤ë¥¸ ìŒì„± ëª¨ë¸ì˜ ê²½ìš° ì´ ë‹¨ê³„ëŠ” Log-Mel spectrogram ì¶”ì¶œì— í•´ë‹¹í•  ìˆ˜ ìˆìŒ.\n",
    "\n",
    "3. transcriptionì„ ì¸ì½”ë”©í•˜ì—¬ label IDë¥¼ ì§€ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # batched output is \"un-batched\"\n",
    "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "    \n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"text\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(prepare_dataset, remove_columns=train_ds.column_names, num_proc=4)\n",
    "test_ds = test_ds.map(prepare_dataset, remove_columns=test_ds.column_names, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation\n",
    "\n",
    "ğŸ¤— Trainer\n",
    "\n",
    "1. define Data Collector : Data CollectorëŠ” pre-processed dataë¥¼ ê°€ì ¸ì™€ì„œ modelì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” PyTorch tensorë¥¼ ì¤€ë¹„\n",
    "\n",
    "2. Evaluation metrics: Evaluation ì¤‘ì— CER ë©”íŠ¸ë¦­ì„ ì‚¬ìš©í•˜ì—¬ modelì„ í‰ê°€. ì´ ê³„ì‚°ì„ ì²˜ë¦¬í•˜ëŠ” compute_metrics í•¨ìˆ˜ë¥¼ ì •ì˜\n",
    "\n",
    "3. Load a pre-trained checkpoint: pre-trained checkpointë¥¼ loadí•˜ê³  í•™ìŠµì„ ìœ„í•´ ì˜¬ë°”ë¥´ê²Œ êµ¬ì„±\n",
    "\n",
    "4. Define the training configuration: ğŸ¤— Trainerì˜ Training scheduleì„ ì •ì˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Data Collector\n",
    "\n",
    "1. input_features\n",
    "    - `input_features`ë¥¼ sequenceì˜ ìµœëŒ€ ê¸¸ì´ê¹Œì§€ íŒ¨ë”©\n",
    "    - `input_features`ë¥¼ PyTorch tensorë¡œ ë³€í™˜ì„ ì§„í–‰ (`return_tensors=pt`)\n",
    "2. labels\n",
    "    - `labels`ë¥¼ sequenceì˜ ìµœëŒ€ ê¸¸ì´ê¹Œì§€ íŒ¨ë”©\n",
    "    - `labels`ë¥¼ PyTorch tensorë¡œ ë³€í™˜ì„ ì§„í–‰ (`return_tensors=pt`)\n",
    "    - attention maskedëœ tokenì€ -100ìœ¼ë¡œ ëŒ€ì²´ë˜ì–´ ì†ì‹¤ì„ ê³„ì‚°í•  ë•Œ í•´ë‹¹ tokenì„ ê³ ë ¤í•˜ì§€ ì•Šë„ë¡ í•¨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        # padding input feature\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        # padding label feature\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Matrics\n",
    "\n",
    "- Character Error Rate(CER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "cer_metric = load_metric(\"cer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelì€ ë‹¤ìŒê³¼ ê°™ì€ return ê°’ì„ ê°€ì§„ë‹¤:\n",
    "$\\mathbf{y}_1, \\ldots, \\mathbf{y}_m$ with $\\mathbf{y}_1 = f_{\\theta}(x_1, \\ldots, x_n)[0]$ and $n >> m$.\n",
    "\n",
    "ë¡œê·¸ ë²¡í„° $\\mathbf{y}_1$ì€ ì•ì„œ ì •ì˜í•œ ì–´íœ˜ì˜ ê° ë‹¨ì–´ì— ëŒ€í•œ log oddë¥¼ í¬í•¨ -> $\\text{len}(\\mathbf{y}_i) =$ `config.vocab_size`\n",
    "\n",
    "ëª¨ë¸ì˜ ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ ì˜ˆì¸¡ì„ ë³´ê¸°ìœ„í•´ ë¡œê·¸ì˜ `argmax(...)` ì ìš©\n",
    "\n",
    "ë˜í•œ ì¸ì½”ë”©ëœ labelì„ ë‹¤ì‹œ ì›ë˜ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ -100ì„ pad_token_idë¡œ ë°”ê¾¸ê³  ì—°ì†ëœ í† í°ì´ CTC ìŠ¤íƒ€ì¼ 1ì—ì„œ ë™ì¼í•œ í† í°ìœ¼ë¡œ ê·¸ë£¹í™”ë˜ì§€ ì•Šë„ë¡ í•˜ë©´ì„œ idë¥¼ ë””ì½”ë”©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"cer\": cer}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a Pre-Trained Checkpoint\n",
    "\n",
    "- pre-trained `XLSR-Wav2Vec2` checkpoint ë¡œë“œ \n",
    "\n",
    "- tokenizerì˜ `pad_token_id`ëŠ” ëª¨ë¸ì˜ `pad_token_id`ë¥¼ ì •ì˜í•˜ê±°ë‚˜ `Wav2Vec2ForCTC`ì˜ ê²½ìš° CTCì˜ *ë¹ˆ í† í°* ${}^2$ë¥¼ ì •ì˜í•´ì•¼ í•©ë‹ˆë‹¤. \n",
    "\n",
    "- GPU ë©”ëª¨ë¦¬ë¥¼ ì ˆì•½í•˜ê¸° ìœ„í•´ íŒŒì´í† ì¹˜ì˜ [ê·¸ë¼ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŠ¸](https://pytorch.org/docs/stable/checkpoint.html)ë¥¼ í™œì„±í™”í•˜ê³  `ctc_loss_reduction`ì„ \"*mean*\"ìœ¼ë¡œ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForCTC\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-large-xlsr-53\", \n",
    "    attention_dropout=0.1,\n",
    "    hidden_dropout=0.1,\n",
    "    feat_proj_dropout=0.0,\n",
    "    mask_time_prob=0.05,\n",
    "    layerdrop=0.1,\n",
    "    ctc_loss_reduction=\"mean\", \n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size=len(processor.tokenizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`XLSR-Wav2Vec2`ì˜ ì²« ë²ˆì§¸ êµ¬ì„± ìš”ì†ŒëŠ” audio signalì—ì„œ ìŒí–¥ì ìœ¼ë¡œ ì˜ë¯¸ ìˆì§€ë§Œ contextì— ë…ë¦½ì ì¸ featureë¥¼ ì¶”ì¶œí•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” CNN layer stackìœ¼ë¡œ êµ¬ì„±\n",
    "\n",
    "modelì˜ ì´ ë¶€ë¶„ì€ ì´ë¯¸ ì‚¬ì „ í›ˆë ¨ ì¤‘ì— ì¶©ë¶„íˆ í›ˆë ¨ë˜ì—ˆê³ , ë…¼ë¬¸ì—ì„œ ì–¸ê¸‰í–ˆë“¯ì´ ë” ì´ìƒ ë¯¸ì„¸ ì¡°ì •í•  í•„ìš”ê°€ ì—†ìŒ \n",
    "\n",
    "ë”°ë¼ì„œ `feature_extractor`ì˜ ëª¨ë“  íŒŒë¼ë¯¸í„°ì— ëŒ€í•´ `requires_grad`ë¥¼ Falseë¡œ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_feature_extractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ `gradient_checkpoint`ë¥¼ í™œì„±í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Training Configuration\n",
    "\n",
    "TrainingArguments [docs](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments).\n",
    "\n",
    "ë§ˆì§€ë§‰ ë‹¨ê³„ì—ì„œëŠ” íŠ¸ë ˆì´ë‹ê³¼ ê´€ë ¨ëœ ëª¨ë“  ë§¤ê°œë³€ìˆ˜ë¥¼ ì •ì˜\n",
    "- `group_by_length`ëŠ” ì…ë ¥ ê¸¸ì´ê°€ ë¹„ìŠ·í•œ í›ˆë ¨ ìƒ˜í”Œì„ í•˜ë‚˜ì˜ ë°°ì¹˜ë¡œ ê·¸ë£¹í™”í•˜ì—¬ í›ˆë ¨ íš¨ìœ¨ì„ í–¥ìƒ(ëª¨ë¸ì„ í†µê³¼í•˜ëŠ” ì“¸ëª¨ì—†ëŠ” íŒ¨ë”© í† í°ì˜ ì „ì²´ ìˆ˜ë¥¼ í¬ê²Œ ì¤„ì„ìœ¼ë¡œì¨ í•™ìŠµ ì‹œê°„ì„ í¬ê²Œ ë‹¨ì¶•)\n",
    "\n",
    "\n",
    "- ì°¸ê³ ì‚¬í•­\n",
    "    - Epoch: EpochëŠ” ì „ì²´ í›ˆë ¨ ë°ì´í„°ì…‹ì´ ì•Œê³ ë¦¬ì¦˜ì„ í•œ ë²ˆ í†µê³¼í•˜ëŠ” ì£¼ê¸°\n",
    "    - Step: Stepì€ í•˜ë‚˜ì˜ ë°°ì¹˜(batch)ê°€ ì•Œê³ ë¦¬ì¦˜ì„ í•œ ë²ˆ í†µê³¼í•˜ëŠ” ê²ƒ (ì „ì²´ë°ì´í„°ìˆ˜/ë°°ì¹˜ì‚¬ì´ì¦ˆ = ì „ì²´ ë°ì´í„°ì…‹ì— ëŒ€í•œ stepìˆ˜)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"./wav2vec2-large-xlsr-korean\",\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=8,\n",
    "  gradient_accumulation_steps=4,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=10,\n",
    "  fp16=True,\n",
    "  save_steps=100,\n",
    "  eval_steps=100,\n",
    "  logging_steps=10,\n",
    "  learning_rate=3e-4,\n",
    "  warmup_steps=500,\n",
    "  save_total_limit=2,\n",
    "  report_to=\"wandb\",\n",
    "  run_name=\"wav2vec2-large-xlsr-korean\",\n",
    "  # push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ëª¨ë¸ì´ í™”ì ë¹„ìœ¨ì— ë…ë¦½ì ì´ ë  ìˆ˜ ìˆë„ë¡ CTCì—ì„œëŠ” ë™ì¼í•œ ì—°ì† í† í°ì„ ë‹¨ì¼ í† í°ìœ¼ë¡œ ê°„ë‹¨íˆ ê·¸ë£¹í™”í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì¸ì½”ë”©ëœ ë ˆì´ë¸”ì€ ëª¨ë¸ì˜ ì˜ˆì¸¡ í† í°ê³¼ ì¼ì¹˜í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ë””ì½”ë”©í•  ë•Œ ê·¸ë£¹í™”í•´ì„œëŠ” ì•ˆ ë˜ë©°, ë”°ë¼ì„œ group_tokens=False ë§¤ê°œ ë³€ìˆ˜ë¥¼ ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤. ì´ ë§¤ê°œ ë³€ìˆ˜ë¥¼ ì „ë‹¬í•˜ì§€ ì•Šìœ¼ë©´ \"hello\"ì™€ ê°™ì€ ë‹¨ì–´ê°€ ì˜ëª» ì¸ì½”ë”©ë˜ì–´ \"helo\"ë¡œ ë””ì½”ë”©ë©ë‹ˆë‹¤.\n",
    "\n",
    "ë¹ˆ í† í°ì„ ì‚¬ìš©í•˜ë©´ ëª¨ë¸ì´ ë‘ l ì‚¬ì´ì— ë¹ˆ í† í°ì„ ê°•ì œë¡œ ì‚½ì…í•˜ì—¬ \"hello\"ì™€ ê°™ì€ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ì˜ \"hello\"ì— ëŒ€í•œ CTC ì¤€ìˆ˜ ì˜ˆì¸¡ì€ [PAD] [PAD] \"h\" \"e\" \"e\" \"l\" \"l\" [PAD] \"l\" \"o\" \"o\" [PAD]ê°€ ë  ê²ƒì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.push_to_hub(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "- kresnik/wav2vec2-large-xlsr-korean [link](https://huggingface.co/kresnik/wav2vec2-large-xlsr-korean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "from datasets import load_dataset\n",
    "import soundfile as sf\n",
    "import torch\n",
    "from jiwer import cer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(\"kresnik/wav2vec2-large-xlsr-korean\")\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"kresnik/wav2vec2-large-xlsr-korean\").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"kresnik/zeroth_korean\", \"clean\")\n",
    "\n",
    "test_ds = ds['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_array(batch):\n",
    "    speech, _ = sf.read(batch[\"file\"])\n",
    "    batch[\"speech\"] = speech\n",
    "    return batch\n",
    "\n",
    "test_ds = test_ds.map(map_to_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_pred(batch):\n",
    "    inputs = processor(batch[\"speech\"], sampling_rate=16000, return_tensors=\"pt\", padding=\"longest\")\n",
    "    input_values = inputs.input_values.to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values).logits\n",
    "\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    transcription = processor.batch_decode(predicted_ids)\n",
    "    batch[\"transcription\"] = transcription\n",
    "    return batch\n",
    "\n",
    "result = test_ds.map(map_to_pred, batched=True, batch_size=16, remove_columns=[\"speech\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ref  : \", result[0][\"text\"])\n",
    "print(\"trans: \", result[0]['transcription'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CER:\", cer(result[\"text\"], result[\"transcription\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
